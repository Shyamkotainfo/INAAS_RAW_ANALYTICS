from pyspark.sql import functions as F
from pyspark.sql import DataFrame

from ingestion.spark_session import get_spark
from execution.dataframe_loader import load_dataframe


class LocalPySparkExecutor:
    """
    Executes LLM-generated PySpark code in a controlled sandbox.
    """

    def __init__(self):
        self.spark = get_spark()

    # --------------------------------------------------
    # Public helpers
    # --------------------------------------------------

    def load_df(self, file_path: str) -> DataFrame:
        """
        Load a DataFrame from the given file path.
        Used by @profile and other non-LLM flows.
        """
        return load_dataframe(self.spark, file_path)

    # --------------------------------------------------
    # Main execution entry point
    # --------------------------------------------------

    def execute(self, file_path: str, pyspark_code: str):
        """
        Execute validated PySpark code generated by the LLM.

        Contract:
        - `df` is preloaded and available
        - `F` (pyspark.sql.functions) is available
        - Code MUST define `result_df`
        - `result_df` MUST be a pyspark.sql.DataFrame
        """

        # ----------------------------------------------
        # Step 1: Load DataFrame
        # ----------------------------------------------
        df = load_dataframe(self.spark, file_path)

        # ----------------------------------------------
        # Step 2: Prepare execution sandbox
        # ----------------------------------------------
        exec_context = {
            "df": df,
            "F": F,
        }

        # ----------------------------------------------
        # Step 3: Execute generated PySpark code
        # ----------------------------------------------
        try:
            exec(pyspark_code, {}, exec_context)
        except Exception as e:
            raise RuntimeError(
                "Error while executing generated PySpark code:\n\n"
                f"{pyspark_code}"
            ) from e

        # ----------------------------------------------
        # Step 4: Enforce execution contract
        # ----------------------------------------------
        if "result_df" not in exec_context:
            raise ValueError(
                "Generated code must define a variable named `result_df`"
            )

        result_df = exec_context["result_df"]

        if not isinstance(result_df, DataFrame):
            raise TypeError(
                f"Generated code returned {type(result_df)}. "
                "Expected pyspark.sql.DataFrame.\n\n"
                f"Generated code:\n{pyspark_code}"
            )

        # ----------------------------------------------
        # Step 5: Return JSON-serializable output
        # ----------------------------------------------
        return [
            row.asDict()
            for row in result_df.limit(100).collect()
        ]
